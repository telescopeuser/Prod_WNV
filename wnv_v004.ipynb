{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WNV Model Evaluation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn import svm\n",
    "from sklearn.utils.validation import check_random_state\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_wnv_raw = pd.read_csv('train.csv', encoding='utf-8') \n",
    "df_wnv_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_wnv_raw = pd.read_csv('train_sam2csv.csv', encoding='utf-8') \n",
    "df_wnv_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_wnv_raw[[\n",
    "# 'IgnConcatTrapSpecies',\n",
    "# 'IgnConcatBlockSpecies',\n",
    "# 'IgnConcatBlockTrap',\n",
    "# 'IgnDate',\n",
    "'Year',\n",
    "'Month',\n",
    "'Day',\n",
    "'nthDay',\n",
    "'nthWeek',\n",
    "# 'IgnAddress',\n",
    "# 'IgnSpecies',\n",
    "'Species-ReCode',\n",
    "# 'IgnBlock',\n",
    "'Block-ReCode',\n",
    "# 'IgnStreet',\n",
    "# 'IgnTrap',\n",
    "'Trap-ReCode',\n",
    "# 'IgnAddressNumberAndStreet',\n",
    "'Latitude',\n",
    "'Longitude',\n",
    "'AddressAccuracy',\n",
    "# 'IgnNumMosquitos',\n",
    "\n",
    "# 'WnvPresent',\n",
    "\n",
    "# 'IgnConcatDateTrapSpecies',\n",
    "# 'IgnDeDup',\n",
    "'NumMosquitosCombined',\n",
    "'NumMosPreTrapSpecies',\n",
    "'WnvPresentPreTrapSpecies',\n",
    "'NumMosPreBlockSpecies',\n",
    "'WnvPresentPreBlockSpecies',\n",
    "'NumMosPreBlockTrap',\n",
    "'WnvPresentPreBlockTrap',\n",
    "'Tmax',\n",
    "'Tmin',\n",
    "'Tavg',\n",
    "'Depart',\n",
    "'DewPoint',\n",
    "'WetBulb',\n",
    "'Heat',\n",
    "'Cool',\n",
    "'Sunrise',\n",
    "'Sunset',\n",
    "'StnPressure',\n",
    "'SeaLevel',\n",
    "'ResultSpeed',\n",
    "'ResultDir',\n",
    "'AvgSpeed',\n",
    "    \n",
    "# 'PreTmax',\n",
    "# 'PreTmin',\n",
    "# 'PreTavg',\n",
    "# 'PreDepart',\n",
    "# 'PreDewPoint',\n",
    "# 'PreWetBulb',\n",
    "# 'PreHeat',\n",
    "# 'PreCool',\n",
    "# 'PreSunrise',\n",
    "# 'PreSunset',\n",
    "# 'PreStnPressure',\n",
    "# 'PreSeaLevel',\n",
    "# 'PreResultSpeed',\n",
    "# 'PreResultDir',\n",
    "# 'PreAvgSpeed',\n",
    "    \n",
    "# 'kmeans8',\n",
    "    \n",
    "# 'DeltaTmax',\n",
    "# 'DeltaTmin',\n",
    "'DeltaTavg',\n",
    "'DeltaDepart',\n",
    "'DeltaDewPoint',\n",
    "'DeltaWetBulb',\n",
    "'DeltaHeat',\n",
    "'DeltaCool',\n",
    "'DeltaSunrise',\n",
    "'DeltaSunset',\n",
    "'DeltaStnPressure',\n",
    "# 'DeltaSeaLevel',\n",
    "# 'DeltaResultSpeed',\n",
    "# 'DeltaResultDir',\n",
    "# 'DeltaAvgSpeed',\n",
    "    \n",
    "# 'SprayWeight',\n",
    "    \n",
    "'WeightNumMosquitosCombined',\n",
    "'WeightNumMosPreTrapSpecies',\n",
    "'WeightNumMosPreBlockSpecies',\n",
    "'WeightNumMosPreBlockTrap',\n",
    "    \n",
    "'DeltaWeightNumMosPreTrapSpecies',\n",
    "'DeltaWeightNumMosPreBlockSpecies',\n",
    "'DeltaWeightNumMosPreBlockTrap'\n",
    "        ]].as_matrix()\n",
    "\n",
    "y = df_wnv_raw[['WnvPresent']].as_matrix().reshape(len(df_wnv_raw),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Data Exploration Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wnv_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def wnv_hist(hist_col):\n",
    "    var1 = df_wnv_raw[df_wnv_raw['WnvPresent'] == 1][hist_col]\n",
    "    var2 = df_wnv_raw[df_wnv_raw['WnvPresent'] != 1][hist_col]\n",
    "    plt.hist(var2, histtype='stepfilled', bins=50, normed=False, color='blue', alpha=0.5, label='WNV Negative')\n",
    "    plt.hist(var1, histtype='stepfilled', bins=50, normed=False, color='red', alpha=0.5, label='WNV Positive')\n",
    "    plt.title(\"Histogram\")\n",
    "    plt.xlabel(hist_col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "#     plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def wnv_mosaic(hist_col):\n",
    "#     plt.rcParams['font.size'] = 12.0\n",
    "    mosaic(df_wnv_raw, [hist_col, 'WnvPresent']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnv_hist(df_wnv_raw.columns[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wnv_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in df_wnv_raw.columns:\n",
    "    try:\n",
    "        wnv_hist(i)\n",
    "    except:\n",
    "        print (\"Ignore column : \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ignore column :  IgnConcatTrapSpecies\n",
    "* Ignore column :  IgnConcatBlockSpecies\n",
    "* Ignore column :  IgnConcatBlockTrap\n",
    "* Ignore column :  IgnDate\n",
    "\n",
    "* Ignore column :  IgnAddress\n",
    "* Ignore column :  IgnSpecies\n",
    "\n",
    "* Ignore column :  IgnStreet\n",
    "* Ignore column :  IgnTrap\n",
    "\n",
    "* Ignore column :  IgnConcatDateTrapSpecies\n",
    "* Ignore column :  IgnDeDup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=\"IgnSpecies\", hue=\"WnvPresent\", data=df_wnv_raw, palette=\"Set2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=\"IgnTrap\", hue=\"WnvPresent\", data=df_wnv_raw, palette=\"Set2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=\"IgnBlock\", hue=\"WnvPresent\", data=df_wnv_raw, palette=\"Set2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = check_random_state(0)\n",
    "n_folds = 6\n",
    "# n_samples, n_features = X.shape\n",
    "# random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GB\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=200, # score: 0.94608 (AUC 0.78), learning_rate=0.0005, max_depth=4, min_samples_split=30, max_features=5\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.66030, default learning_rate=0.1\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.88464 (AUC 0.80), learning_rate=0.0035\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.90691 (AUC 0.80), learning_rate=0.002\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0005, max_depth=5, min_samples_split=20\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0005, max_depth=5, min_samples_split=30\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0005, max_depth=5, min_samples_split=30, max_features=10\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0005, max_depth=5, min_samples_split=30, max_features=5\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0005, max_depth=4, min_samples_split=30, max_features=5\n",
    "classifier_GB = GradientBoostingClassifier(n_estimators=1000, # score: 0.79112 (AUC 0.70), learning_rate=0.0035, max_depth=5, min_samples_split=30, max_features=10\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=1000, # score: 0.94608, learning_rate=0.0005, max_depth=5, min_samples_split=30, max_features=5\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=1000, # score: 0.63623, default learning_rate=0.1\n",
    "# classifier_GB = GradientBoostingClassifier(n_estimators=2000, # score: 0.77567, learning_rate=0.0035\n",
    "#                                    loss='deviance',\n",
    "#                                    subsample=1,\n",
    "                                   learning_rate=0.0035,\n",
    "                                   max_features=10,\n",
    "                                   max_depth=5,\n",
    "                                   min_samples_split=30,\n",
    "                                   random_state=rng)\n",
    "\n",
    "cv = cross_val_score(classifier_GB,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('GB score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AB\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0035\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=100, # score: 0.94608 (AUC 0.77), learning_rate=0.002\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=500, # score: 0.94608 (AUC 0.85), learning_rate=0.002\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=500, # score: 0.94608, learning_rate=0.0005\n",
    "classifier_AB = AdaBoostClassifier(n_estimators=1000, # score: 0.94608 (AUC 0.88), learning_rate=0.0035 <<< Best\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=1500, # score: 0.92686 (AUC 0.88), learning_rate=0.0035 \n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=2000, # score: 0.63941, default learning_rate=0.1\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=2000, # score: 0.90117, learning_rate=0.0035\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=2000, # score: 0.89056 (AUC 0.88), learning_rate=0.004\n",
    "# classifier_AB = AdaBoostClassifier(n_estimators=2000, # score: 0.83374 (AUC 0.83), learning_rate=0.01\n",
    "                                   learning_rate=0.0035,\n",
    "                                   random_state=rng)\n",
    "\n",
    "cv = cross_val_score(classifier_AB,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('AB CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.90469, max_depth=5, min_samples_split=20,\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.90540, max_depth=5, min_samples_split=30,\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.93005, max_depth=4, min_samples_split=30,\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.93099, max_depth=4, min_samples_split=40,\n",
    "classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.93794 (AUC 0.82), max_depth=3, min_samples_split=20,\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=200, # score: 0.93771, max_depth=3, min_samples_split=20,\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=1000, # score: 0.90493, max_depth=5, min_samples_split=30,\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.88900, max_depth=5, min_samples_split=30, max_features=10\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=1000, # score: 0.88864, max_depth=5, min_samples_split=30, max_features=10\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=500, # score: 0.77154\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=1000, # score: 0.76469\n",
    "# classifier_RF = RandomForestClassifier(n_estimators=2000, # score: 0.76564\n",
    "#                                     max_features=10,\n",
    "                                    max_depth=3,\n",
    "                                    min_samples_split=20,\n",
    "                                    random_state=rng)\n",
    "\n",
    "cv = cross_val_score(classifier_RF,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('RF CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=500, # score: 0.70973\n",
    "classifier_ET = ExtraTreesClassifier(n_estimators=500, # score: 0.93382 (AUC 0.81), max_depth=5, min_samples_split=30, max_features=10\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=1000, # score: 0.93276, max_depth=5, min_samples_split=30, max_features=10\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=1000, # score: 0.94572, max_depth=4, min_samples_split=30, max_features=5\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=1000, # score: 0.94608 (AUC 0.82), max_depth=3, min_samples_split=30, max_features=5\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=1000, # score: 0.93241, max_depth=5, min_samples_split=20, max_features=10\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=1000, # score: 0.71067\n",
    "# classifier_ET = ExtraTreesClassifier(n_estimators=2000, # score: 0.71149\n",
    "                                    max_depth=5,\n",
    "                                    min_samples_split=30,\n",
    "                                    max_features=10,\n",
    "                                    random_state=rng)\n",
    "\n",
    "cv = cross_val_score(classifier_ET,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('ET CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BG\n",
    "# classifier_BG = BaggingClassifier(n_estimators=500, # score: 0.77035, max_features=20\n",
    "# classifier_BG = BaggingClassifier(n_estimators=500, # score: 0.78085, max_features=10\n",
    "# classifier_BG = BaggingClassifier(n_estimators=200, # score: 0.77707, max_features=10\n",
    "classifier_BG = BaggingClassifier(n_estimators=1000, # score: 0.78096 (AUC 0.56), max_features=10\n",
    "# classifier_BG = BaggingClassifier(n_estimators=500, # score: 0.76553, max_features=5\n",
    "# classifier_BG = BaggingClassifier(n_estimators=500, # score: 0.70181\n",
    "# classifier_BG = BaggingClassifier(n_estimators=1000, # score: 0.69779\n",
    "# classifier_BG = BaggingClassifier(n_estimators=2000, # score: 0.70004\n",
    "                                    max_features=10,\n",
    "                                    random_state=rng)\n",
    "\n",
    "cv = cross_val_score(classifier_BG,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('BG CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "classifier_LR = LogisticRegression(random_state=rng) # score: 0.89562 (AUC 0.75)\n",
    "cv = cross_val_score(classifier_LR,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('LR CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC Liner\n",
    "classifier_SVCL = svm.SVC(kernel='linear', probability=True, random_state=rng) # score: 0.89197 (AUC 0.78)\n",
    "cv = cross_val_score(classifier_SVCL,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('SVC Liner CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC RBF\n",
    "classifier_SVCR = svm.SVC(kernel='rbf', probability=True, random_state=rng) # score: 0.94608 (AUC 0.61)\n",
    "cv = cross_val_score(classifier_SVCR,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('SVC RBF CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=11) # score: 0.93794 (AUC 0.60)\n",
    "cv = cross_val_score(classifier_KNN,\n",
    "                            X,\n",
    "                            y,\n",
    "                            cv=StratifiedKFold(n_folds))\n",
    "print('KNN CV score: {0:.5f}'.format(cv.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# West Nile Virus Evaluation: AUC in ROC Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(classifier):\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "    cv = StratifiedKFold(n_splits=6)\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\n",
    "    lw = 2\n",
    "\n",
    "    i = 0\n",
    "    for (train, test), color in zip(cv.split(X, y), colors):\n",
    "        probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=lw, color=color,\n",
    "                 label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n",
    "             label='Luck')\n",
    "\n",
    "    mean_tpr /= cv.get_n_splits(X, y)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_ET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_BG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_SVCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(classifier_SVCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find best 'n_neighbors':\n",
    "import operator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "n_neighbors_best_list = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "for n_neighbors in [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]:\n",
    "    knn = KNeighborsClassifier(n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(n_neighbors, knn.score(X_test, y_test))\n",
    "    n_neighbors_best_list.append(knn.score(X_test, y_test))\n",
    "\n",
    "n_neighbors_best, n_neighbors_value = max(enumerate(n_neighbors_best_list), key=operator.itemgetter(1))\n",
    "print('\\nBest n_neighbors is: %d \\nwith value : %f' % (n_neighbors_best, n_neighbors_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(KNeighborsClassifier(n_neighbors=n_neighbors_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation\n",
    "\n",
    "Here we've used 2-fold cross-validation. This is just one specialization of $K$-fold cross-validation, where we split the data into $K$ chunks and perform $K$ fits, where each chunk gets a turn as the validation set.\n",
    "We can do this by changing the ``cv`` parameter above. Let's do 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(1), X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an even better idea of how well our model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone over the basics of validation, and cross-validation, it's time to go into even more depth regarding model selection.\n",
    "\n",
    "The issues associated with validation and \n",
    "cross-validation are some of the most important\n",
    "aspects of the practice of machine learning.  Selecting the optimal model\n",
    "for your data is vital, and is a piece of the problem that is not often\n",
    "appreciated by machine learning practitioners.\n",
    "\n",
    "Of core importance is the following question:\n",
    "\n",
    "**If our estimator is underperforming, how should we move forward?**\n",
    "\n",
    "- Use simpler or more complicated model?\n",
    "- Add more features to each observed data point?\n",
    "- Add more training samples?\n",
    "\n",
    "The answer is often counter-intuitive.  In particular, **Sometimes using a\n",
    "more complicated model will give _worse_ results.**  Also, **Sometimes adding\n",
    "training data will not improve your results.**  The ability to determine\n",
    "what steps will improve your model is what separates the successful machine\n",
    "learning practitioners from the unsuccessful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of the Bias-Variance Tradeoff\n",
    "\n",
    "For this section, we'll work with a simple 1D regression problem.  This will help us to\n",
    "easily visualize the data and the model, and the results generalize easily to  higher-dimensional\n",
    "datasets.  We'll explore a simple **linear regression** problem.\n",
    "This can be accomplished within scikit-learn with the `sklearn.linear_model` module.\n",
    "\n",
    "We'll create a simple nonlinear function that we'd like to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_func(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a realization of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    # randomly sample the data\n",
    "    np.random.seed(1)\n",
    "    X = np.random.random(N)[:, np.newaxis]\n",
    "    y = test_func(X.ravel(), error)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_data(40, error=1)\n",
    "plt.scatter(X.ravel(), y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say we want to perform a regression on this data.  Let's use the built-in linear regression function to compute a fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fit a straight line to the data, but clearly this model is not a good choice.  We say that this model is **biased**, or that it **under-fits** the data.\n",
    "\n",
    "Let's try to improve this by creating a more complicated model.  We can do this by adding degrees of freedom, and computing a polynomial regression over the inputs. Scikit-learn makes this easy with the ``PolynomialFeatures`` preprocessor, which can be pipelined with a linear regression.\n",
    "\n",
    "Let's make a convenience routine to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use this to fit a quadratic curve to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces the mean squared error, and makes a much better fit.  What happens if we use an even higher-degree polynomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(30)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "plt.ylim(-4, 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase the degree to this extent, it's clear that the resulting fit is no longer reflecting the true underlying distribution, but is more sensitive to the noise in the training data. For this reason, we call it a **high-variance model**, and we say that it **over-fits** the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's use IPython's interact capability (only in IPython 2.0+) to explore this interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "\n",
    "def plot_fit(degree=1, Npts=50):\n",
    "    X, y = make_data(Npts, error=1)\n",
    "    X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "    \n",
    "    model = PolynomialRegression(degree=degree)\n",
    "    model.fit(X, y)\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    plt.scatter(X.ravel(), y)\n",
    "    plt.plot(X_test.ravel(), y_test)\n",
    "    plt.ylim(-4, 14)\n",
    "    plt.title(\"mean squared error: {0:.2f}\".format(mean_squared_error(model.predict(X), y)))\n",
    "    \n",
    "interact(plot_fit, degree=[1, 30], Npts=[2, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Over-fitting with Validation Curves\n",
    "\n",
    "Clearly, computing the error on the training data is not enough (we saw this previously). As above, we can use **cross-validation** to get a better handle on how the model fit is working.\n",
    "\n",
    "Let's do this here, again using the ``validation_curve`` utility. To make things more clear, we'll use a slightly larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_data(120, error=1.0)\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "def rms_error(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "\n",
    "degree = np.arange(0, 18)\n",
    "val_train, val_test = validation_curve(PolynomialRegression(), X, y,\n",
    "                                       'polynomialfeatures__degree', degree, cv=7,\n",
    "                                       scoring=rms_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the validation curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_with_err(x, data, **kwargs):\n",
    "    mu, std = data.mean(1), data.std(1)\n",
    "    lines = plt.plot(x, mu, '-', **kwargs)\n",
    "    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                     facecolor=lines[0].get_color(), alpha=0.2)\n",
    "\n",
    "plot_with_err(degree, val_train, label='training scores')\n",
    "plot_with_err(degree, val_test, label='validation scores')\n",
    "plt.xlabel('degree'); plt.ylabel('rms error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the trend here, which is common for this type of plot.\n",
    "\n",
    "1. For a small model complexity, the training error and validation error are very similar. This indicates that the model is **under-fitting** the data: it doesn't have enough complexity to represent the data. Another way of putting it is that this is a **high-bias** model.\n",
    "\n",
    "2. As the model complexity grows, the training and validation scores diverge. This indicates that the model is **over-fitting** the data: it has so much flexibility, that it fits the noise rather than the underlying trend. Another way of putting it is that this is a **high-variance** model.\n",
    "\n",
    "3. Note that the training score (nearly) always improves with model complexity. This is because a more complicated model can fit the noise better, so the model improves. The validation data generally has a sweet spot, which here is around 5 terms.\n",
    "\n",
    "Here's our best-fit model according to the cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(4).fit(X, y)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_test, model.predict(X_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Data Sufficiency with Learning Curves\n",
    "\n",
    "As you might guess, the exact turning-point of the tradeoff between bias and variance is highly dependent on the number of training points used.  Here we'll illustrate the use of *learning curves*, which display this property.\n",
    "\n",
    "The idea is to plot the mean-squared-error for the training and test set as a function of *Number of Training Points*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def plot_learning_curve(degree=3):\n",
    "    train_sizes = np.linspace(0.05, 1, 20)\n",
    "    N_train, val_train, val_test = learning_curve(PolynomialRegression(degree),\n",
    "                                                  X, y, train_sizes, cv=5,\n",
    "                                                  scoring=rms_error)\n",
    "    plot_with_err(N_train, val_train, label='training scores')\n",
    "    plot_with_err(N_train, val_test, label='validation scores')\n",
    "    plt.xlabel('Training Set Size'); plt.ylabel('rms error')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.xlim(5, 80)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the learning curves look like for a linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a typical learning curve: for very few training points, there is a large separation between the training and test error, which indicates **over-fitting**.  Given the same model, for a large number of training points, the training and testing errors converge, which indicates potential **under-fitting**.\n",
    "\n",
    "As you add more data points, the training error will never increase, and the testing error will never decrease (why do you think this is?)\n",
    "\n",
    "It is easy to see that, in this plot, if you'd like to reduce the MSE down to the nominal value of 1.0 (which is the magnitude of the scatter we put in when constructing the data), then adding more samples will *never* get you there.  For $d=1$, the two curves have converged and cannot move lower. What about for a larger value of $d$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that by adding more model complexity, we've managed to lower the level of convergence to an rms error of 1.0!\n",
    "\n",
    "What if we get even more complex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an even more complex model, we still converge, but the convergence only happens for *large* amounts of training data.\n",
    "\n",
    "So we see the following:\n",
    "\n",
    "- you can **cause the lines to converge** by adding more points or by simplifying the model.\n",
    "- you can **bring the convergence error down** only by increasing the complexity of the model.\n",
    "\n",
    "Thus these curves can give you hints about how you might improve a sub-optimal model. If the curves are already close together, you need more model complexity. If the curves are far apart, you might also improve the model by adding more data.\n",
    "\n",
    "To make this more concrete, imagine some telescope data in which the results are not robust enough.  You must think about whether to spend your valuable telescope time observing *more objects* to get a larger training set, or *more attributes of each object* in order to improve the model.  The answer to this question has real consequences, and can be addressed using these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've gone over several useful tools for model validation\n",
    "\n",
    "- The **Training Score** shows how well a model fits the data it was trained on. This is not a good indication of model effectiveness\n",
    "- The **Validation Score** shows how well a model fits hold-out data. The most effective method is some form of cross-validation, where multiple hold-out sets are used.\n",
    "- **Validation Curves** are a plot of validation score and training score as a function of **model complexity**:\n",
    "  + when the two curves are close, it indicates *underfitting*\n",
    "  + when the two curves are separated, it indicates *overfitting*\n",
    "  + the \"sweet spot\" is in the middle\n",
    "- **Learning Curves** are a plot of the validation score and training score as a function of **Number of training samples**\n",
    "  + when the curves are close, it indicates *underfitting*, and adding more data will not generally improve the estimator.\n",
    "  + when the curves are far apart, it indicates *overfitting*, and adding more data may increase the effectiveness of the model.\n",
    "  \n",
    "These tools are powerful means of evaluating your model on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\n",
    "lw = 2\n",
    "\n",
    "i = 0\n",
    "for (train, test), color in zip(cv.split(X, y), colors):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color,\n",
    "             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',\n",
    "         label='Luck')\n",
    "\n",
    "mean_tpr /= cv.get_n_splits(X, y)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',\n",
    "         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
